{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd832c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'raw_date': ['Mon, 28th May 2020 12:34:56 GMT', 'Tue, 1st Jan 2019 05:00:00 GMT']\n",
    "})\n",
    "\n",
    "# Remove day suffixes like 'st', 'nd', 'rd', 'th' using regex\n",
    "df['cleaned_date'] = df['raw_date'].apply(lambda x: re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', x))\n",
    "\n",
    "# Parse into datetime format\n",
    "df['parsed_date'] = pd.to_datetime(df['cleaned_date'], format='%a, %d %b %Y %H:%M:%S %Z', errors='coerce')\n",
    "\n",
    "# Optional: just keep the date part (no time)\n",
    "df['just_date'] = df['parsed_date'].dt.date\n",
    "\n",
    "print(df[['raw_date', 'just_date']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "years = ['2010', '2025']\n",
    "car1_prices = [15000, 18000]\n",
    "car2_prices = [20000, 25000]\n",
    "car3_prices = [17000, 22000]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Stacked bars\n",
    "bar1 = ax.bar(years, car1_prices, label='Car 1')\n",
    "bar2 = ax.bar(years, car2_prices, bottom=car1_prices, label='Car 2')\n",
    "\n",
    "# Compute bottoms for car 3 (stacked on car1 + car2)\n",
    "car1_plus_car2 = [c1 + c2 for c1, c2 in zip(car1_prices, car2_prices)]\n",
    "bar3 = ax.bar(years, car3_prices, bottom=car1_plus_car2, label='Car 3')\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_ylabel('Total Price ($)')\n",
    "ax.set_title('Stacked Car Prices in 2010 vs 2025')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40342b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get SHAP interaction values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_interaction_values = explainer.shap_interaction_values(X)  # shape: [n_samples, n_features, n_features]\n",
    "\n",
    "# Define feature of interest\n",
    "feature_name = 'A'\n",
    "feature_index = list(X.columns).index(feature_name)\n",
    "\n",
    "# Interaction values of 'A' with every other feature (averaged across samples)\n",
    "interaction_strengths = np.abs(shap_interaction_values[:, feature_index, :]).mean(axis=0)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "interaction_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'interaction_strength': interaction_strengths\n",
    "}).sort_values(by='interaction_strength', ascending=False)\n",
    "\n",
    "# Optional: Drop self-interaction if you only want cross-feature effects\n",
    "interaction_df = interaction_df[interaction_df.feature != feature_name]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=interaction_df, x='interaction_strength', y='feature')\n",
    "plt.title(f\"Mean SHAP Interaction Strengths with '{feature_name}'\")\n",
    "plt.xlabel(\"Mean |SHAP Interaction Value|\")\n",
    "plt.ylabel(\"Interacting Feature\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12926ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMAKE_ARGS=\"-DLLAMA_CUDA=OFF -DLLAMA_METAL=OFF -DLLAMA_CLBLAST=OFF -DLLAMA_BLAS=OFF\" \\\n",
    "pip install --force-reinstall --no-cache-dir --no-binary llama-cpp-python llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0441d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# --- Step 1: Simulate weekly purchase data ---\n",
    "np.random.seed(0)\n",
    "weeks = pd.date_range(\"2023-01-01\", periods=52, freq='W')\n",
    "states = ['Texas', 'Oklahoma', 'Louisiana', 'California', 'New York']\n",
    "\n",
    "def make_trend(base, noise=5):\n",
    "    return base + np.random.normal(0, noise, size=len(weeks))\n",
    "\n",
    "data = {\n",
    "    'Texas': make_trend(np.linspace(100, 200, 52)),\n",
    "    'Oklahoma': make_trend(np.linspace(90, 180, 52)),\n",
    "    'Louisiana': make_trend(np.linspace(105, 205, 52)),\n",
    "    'California': make_trend(np.linspace(300, 400, 52)),\n",
    "    'New York': make_trend(np.linspace(200, 100, 52)),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=weeks)\n",
    "\n",
    "# --- Step 2: Normalize trends (Z-score) ---\n",
    "df_normalized = df.apply(zscore)\n",
    "\n",
    "# --- Step 3: Hyper-tune K using silhouette score ---\n",
    "X = df_normalized.T  # states as rows\n",
    "sil_scores = {}\n",
    "for k in range(2, min(len(X), 10)):\n",
    "    model = KMeans(n_clusters=k, random_state=0)\n",
    "    labels = model.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    sil_scores[k] = score\n",
    "\n",
    "best_k = max(sil_scores, key=sil_scores.get)\n",
    "print(f\"Best number of clusters: {best_k} with silhouette score: {sil_scores[best_k]:.3f}\")\n",
    "\n",
    "# --- Step 4: Fit final model and visualize ---\n",
    "final_model = KMeans(n_clusters=best_k, random_state=0)\n",
    "labels = final_model.fit_predict(X)\n",
    "cluster_df = pd.DataFrame({'state': X.index, 'cluster': labels})\n",
    "print(cluster_df)\n",
    "\n",
    "# --- Plot silhouette scores ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(list(sil_scores.keys()), list(sil_scores.values()), marker='o')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs. Number of Clusters\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(2, min(len(X), 10)))\n",
    "plt.axvline(x=best_k, linestyle='--', color='red', label=f'Best k = {best_k}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Step 5: Plot each cluster's normalized time trends ---\n",
    "for cluster in cluster_df['cluster'].unique():\n",
    "    cluster_states = cluster_df[cluster_df['cluster'] == cluster]['state']\n",
    "    df_normalized[cluster_states].plot(title=f\"Cluster {cluster} Trends\")\n",
    "    plt.ylabel(\"Z-Score Normalized Purchases\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Step 6: Correlation heatmap ---\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Raw Correlation of Purchase Trends\")\n",
    "plt.show()\n",
    "\n",
    "# --- Step 7: Intra-cluster correlation check ---\n",
    "def mean_cluster_corr(corr_matrix, cluster_df):\n",
    "    results = {}\n",
    "    for cluster in cluster_df['cluster'].unique():\n",
    "        members = cluster_df[cluster_df['cluster'] == cluster]['state']\n",
    "        intra = corr_matrix.loc[members, members].values\n",
    "        intra_corr = intra[np.triu_indices_from(intra, k=1)]\n",
    "        results[f\"Cluster {cluster} mean corr\"] = np.mean(intra_corr)\n",
    "    return results\n",
    "\n",
    "print(mean_cluster_corr(corr, cluster_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to datetime if they aren't already\n",
    "df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "df['end_date']   = pd.to_datetime(df['end_date'])\n",
    "\n",
    "# 1ï¸âƒ£ Recode: 1 if end_date is after start_date, else 0\n",
    "df['date_after_flag'] = (df['end_date'] > df['start_date']).astype(int)\n",
    "\n",
    "# 2ï¸âƒ£ Recode: 1 if 'comments' contains 'hi' (case-insensitive), else 0\n",
    "df['contains_hi_flag'] = df['comments'].str.contains('hi', case=False, na=False).astype(int)\n",
    "\n",
    "# Sort so earliest date comes first\n",
    "df = df.sort_values(by=['email', 'date'])\n",
    "\n",
    "# Drop duplicates, keeping first occurrence (earliest date)\n",
    "df_first_date = df.drop_duplicates(subset='email', keep='first')\n",
    "\n",
    "# 1ï¸âƒ£ Boolean masks\n",
    "mask_saiid = df['campaign'].str.contains('saiid', case=False, na=False)\n",
    "mask_not_saiid = ~mask_saiid\n",
    "\n",
    "# 2ï¸âƒ£ Sets of emails\n",
    "emails_saiid = set(df.loc[mask_saiid, 'email'])\n",
    "emails_not_saiid = set(df.loc[mask_not_saiid, 'email'])\n",
    "\n",
    "# 3ï¸âƒ£ Intersection and differences\n",
    "emails_both = emails_saiid & emails_not_saiid\n",
    "emails_only_saiid = emails_saiid - emails_both\n",
    "emails_only_not_saiid = emails_not_saiid - emails_both\n",
    "\n",
    "# 4ï¸âƒ£ Counts\n",
    "print(\"Count in both:\", len(emails_both))\n",
    "print(\"Count only in saiid:\", len(emails_only_saiid))\n",
    "print(\"Count only in non-saiid:\", len(emails_only_not_saiid))\n",
    "\n",
    "# Conditional logic\n",
    "df['final_flag'] = (\n",
    "    ((df['one_column'] == 1) & (df['min_date'] > df['application_date'])) |\n",
    "    ((df['one_column'] == 0) & (df['application_date'] > fixed_date))\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bf293",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_substr = 'hi'  # the substring to search for\n",
    "\n",
    "# 1) Which rows are in a matching campaign?\n",
    "mask = df['campaign'].astype(str).str.contains(campaign_substr, case=False, na=False)\n",
    "\n",
    "# 2) All emails that EVER had a matching campaign\n",
    "emails_with_match = set(df.loc[mask, 'email'])\n",
    "\n",
    "# 3) Flag every row for those emails (1 if the email ever used the campaign, else 0)\n",
    "df['ever_hi'] = df['email'].isin(emails_with_match).astype(int)\n",
    "\n",
    "# 4) First date each email was associated with a matching campaign\n",
    "first_hi_per_email = (\n",
    "    df.loc[mask]\n",
    "      .groupby('email', as_index=False)['date']\n",
    "      .min()\n",
    "      .set_index('email')['date']\n",
    ")\n",
    "\n",
    "# Map to every row; emails without a match get NaT\n",
    "df['first_hi_date'] = df['email'].map(first_hi_per_email)\n",
    "\n",
    "# (optional) If you'd rather fill non-matching emails with a sentinel string:\n",
    "# df['first_hi_date'] = df['first_hi_date'].dt.strftime('%Y-%m-%d').fillna('never')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by score and calculate average sales rate\n",
    "avg_sales = df.groupby(\"score\")[\"actual\"].mean().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(avg_sales[\"score\"], avg_sales[\"actual\"], marker=\"o\", linewidth=2)\n",
    "\n",
    "# Style\n",
    "plt.title(\"Average Sales Rate by Model Score\", fontsize=14)\n",
    "plt.xlabel(\"Model Score (1 = high sale likelihood, 5 = low)\", fontsize=12)\n",
    "plt.ylabel(\"Average Sales Rate\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(avg_sales[\"score\"])\n",
    "\n",
    "# Annotate values\n",
    "for x, y in zip(avg_sales[\"score\"], avg_sales[\"actual\"]):\n",
    "    plt.text(x, y + 0.01, f\"{y:.2%}\", ha=\"center\", fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1df410",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prefixes = (\"1\", \"2\", \"3\")  # numbers you want to match at the start\n",
    "\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=json_file)\n",
    "        file_content = obj['Body'].read().decode('utf-8')\n",
    "\n",
    "        try:\n",
    "            json_data = json.loads(file_content)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"File {json_file} is not valid JSON. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if isinstance(json_data, dict):\n",
    "            json_data = [json_data]\n",
    "        elif not isinstance(json_data, list):\n",
    "            print(f\"Unexpected JSON format in {json_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Filter JSON objects by key values starting with target numbers\n",
    "        filtered_data = [\n",
    "            item for item in json_data\n",
    "            if any(str(v).startswith(target_prefixes) for v in item.values())\n",
    "        ]\n",
    "\n",
    "        all_data.extend(filtered_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {json_file}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import ijson\n",
    "import orjson\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from botocore.config import Config\n",
    "from io import BytesIO\n",
    "\n",
    "# ==== CONFIG ====\n",
    "bucket_name = 'your-bucket-name'\n",
    "prefix = 'your-folder-name/'      # keep trailing slash\n",
    "max_workers = 12                  # adjust to your network/CPU\n",
    "max_pool_connections = 48\n",
    "region = None                     # e.g., \"us-west-2\" or None to use default\n",
    "profile = None                    # e.g., \"your-aws-profile\" or None to use env/default\n",
    "# ===============\n",
    "\n",
    "cfg = Config(max_pool_connections=max_pool_connections,\n",
    "             retries={'max_attempts': 8, 'mode': 'standard'})\n",
    "\n",
    "session = (boto3.session.Session(profile_name=profile, region_name=region)\n",
    "           if profile or region else boto3.session.Session())\n",
    "s3 = session.client('s3', config=cfg)\n",
    "\n",
    "def list_json_keys(bucket: str, pfx: str):\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_it = paginator.paginate(Bucket=bucket, Prefix=pfx)\n",
    "    exts = ('.json', '.jsonl', '.ndjson')\n",
    "    out = []\n",
    "    for page in page_it:\n",
    "        for obj in page.get('Contents', []):\n",
    "            k = obj['Key']\n",
    "            if k.endswith(exts):\n",
    "                out.append(k)\n",
    "    return out\n",
    "\n",
    "def parse_json_body(body_bytes: bytes):\n",
    "    \"\"\"\n",
    "    Return a list[dict] from body_bytes.\n",
    "    Tries JSONL -> JSON array -> single dict -> double-encoded.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    # --- Try JSON Lines first\n",
    "    bio = BytesIO(body_bytes)\n",
    "    saw_jsonl = False\n",
    "    for raw in bio:\n",
    "        line = raw.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        saw_jsonl = True\n",
    "        try:\n",
    "            rec = orjson.loads(line)\n",
    "            if isinstance(rec, dict):\n",
    "                rows.append(rec)\n",
    "            elif isinstance(rec, list):\n",
    "                rows.extend([x for x in rec if isinstance(x, dict)])\n",
    "            # ignore scalars\n",
    "        except orjson.JSONDecodeError:\n",
    "            saw_jsonl = False\n",
    "            break\n",
    "    if saw_jsonl and rows:\n",
    "        return rows\n",
    "    elif saw_jsonl:\n",
    "        # looked like lines but none were dicts; fall through to whole parse\n",
    "        pass\n",
    "\n",
    "    # --- Try top-level array (streaming)\n",
    "    bio.seek(0)\n",
    "    try:\n",
    "        for item in ijson.items(bio, 'item'):\n",
    "            if isinstance(item, dict):\n",
    "                rows.append(item)\n",
    "        if rows:\n",
    "            return rows\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- Whole-file parse (dict or list, possibly double-encoded)\n",
    "    try:\n",
    "        whole = orjson.loads(body_bytes)\n",
    "    except orjson.JSONDecodeError:\n",
    "        # Sometimes the body is actually text; try decoding then parsing\n",
    "        try:\n",
    "            whole = orjson.loads(body_bytes.decode('utf-8', errors='ignore'))\n",
    "        except orjson.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "    # Double-encoded JSON: first parse yields a str\n",
    "    if isinstance(whole, str):\n",
    "        try:\n",
    "            whole = orjson.loads(whole)\n",
    "        except orjson.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "    if isinstance(whole, dict):\n",
    "        # Common container keys; if absent, treat as single record\n",
    "        for k in ('items', 'records', 'data', 'rows', 'result'):\n",
    "            v = whole.get(k)\n",
    "            if isinstance(v, list):\n",
    "                rows.extend([x for x in v if isinstance(x, dict)])\n",
    "                break\n",
    "        else:\n",
    "            rows.append(whole)\n",
    "    elif isinstance(whole, list):\n",
    "        rows.extend([x for x in whole if isinstance(x, dict)])\n",
    "\n",
    "    return rows\n",
    "\n",
    "def process_key(key: str):\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        return parse_json_body(body)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {key}: {e}\")\n",
    "        return []\n",
    "\n",
    "# ---- Run ----\n",
    "keys = list_json_keys(bucket_name, prefix)\n",
    "print(f\"[INFO] Found {len(keys)} files under s3://{bucket_name}/{prefix}\")\n",
    "\n",
    "dfs = []\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futures = [ex.submit(process_key, k) for k in keys]\n",
    "    for fut in as_completed(futures):\n",
    "        rows = fut.result()\n",
    "        if rows:\n",
    "            dfs.append(pd.DataFrame.from_records(rows))\n",
    "\n",
    "if dfs:\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"[INFO] DataFrame created:\", df.shape)\n",
    "    print(df.head())\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    print(\"[INFO] No JSON objects parsed into rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028477ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, gzip, shutil, os\n",
    "\n",
    "def download_s3_csv(bucket, key, out_dir='.', aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name=None):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        aws_session_token=aws_session_token,\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    gz_path = os.path.join(out_dir, os.path.basename(key))\n",
    "    csv_path = gz_path[:-3]\n",
    "\n",
    "    s3.download_file(bucket, key, gz_path)\n",
    "    with gzip.open(gz_path, 'rb') as f_in, open(csv_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    return csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "ðŸ¥‡ 1. Word Frequency Counter\n",
    "\n",
    "Prompt:\n",
    "Given a sentence (string), return a dictionary mapping each word to the number of times it appears.\n",
    "Ignore capitalization and punctuation (.,!?).\n",
    "\n",
    "\n",
    "Test cases:\n",
    "\n",
    "print(word_frequency(\"This is a test. This test is simple.\"))\n",
    "# {'this': 2, 'is': 2, 'a': 1, 'test': 2, 'simple': 1}\n",
    "\n",
    "print(word_frequency(\"Hello hello world!\"))\n",
    "# {'hello': 2, 'world': 1}\n",
    "\n",
    "ðŸ¥ˆ 2. Invert a Dictionary\n",
    "\n",
    "Prompt:\n",
    "Write a function that takes a dictionary d and returns a new dictionary where each value becomes a key, and each key is added to a list of keys that shared that value.\n",
    "\n",
    "Test cases:\n",
    "\n",
    "print(invert_dict({\"a\":1, \"b\":2, \"c\":1}))\n",
    "# {1: ['a', 'c'], 2: ['b']}\n",
    "\n",
    "print(invert_dict({\"x\":3, \"y\":3, \"z\":4}))\n",
    "# {3: ['x', 'y'], 4: ['z']}\n",
    "\n",
    "ðŸ¥‰ 3. Character Counter\n",
    "\n",
    "Prompt:\n",
    "Given a string s, return a dictionary mapping each letter (case-insensitive) to the number of times it appears.\n",
    "Ignore non-alphabetical characters.\n",
    "\n",
    "\n",
    "Test cases:\n",
    "\n",
    "print(char_count(\"Hello World!\"))\n",
    "# {'h': 1, 'e': 1, 'l': 3, 'o': 2, 'w': 1, 'r': 1, 'd': 1}\n",
    "\n",
    "print(char_count(\"AaBbCc\"))\n",
    "# {'a': 2, 'b': 2, 'c': 2}\n",
    "\n",
    "ðŸ… 4. Group by First Letter\n",
    "\n",
    "Prompt:\n",
    "Given a list of words, group them by their first letter (case-sensitive).\n",
    "Return a dictionary where keys are letters and values are lists of words starting with that letter.\n",
    "\n",
    "Test cases:\n",
    "\n",
    "print(group_by_first_letter([\"apple\",\"ant\",\"banana\",\"bat\",\"cat\"]))\n",
    "# {'a': ['apple', 'ant'], 'b': ['banana', 'bat'], 'c': ['cat']}\n",
    "\n",
    "print(group_by_first_letter([\"dog\", \"deer\", \"cat\"]))\n",
    "# {'d': ['dog', 'deer'], 'c': ['cat']}\n",
    "\n",
    "ðŸ† 5. Merge Two Dictionaries with Summed Values\n",
    "\n",
    "Prompt:\n",
    "You are given two dictionaries with numeric values.\n",
    "Merge them so that if a key exists in both, their values are summed.\n",
    "Return a new dictionary.\n",
    "\n",
    "\n",
    "Test cases:\n",
    "\n",
    "print(merge_sum({\"a\":1,\"b\":2}, {\"b\":3,\"c\":4}))\n",
    "# {'a': 1, 'b': 5, 'c': 4}\n",
    "\n",
    "print(merge_sum({}, {\"x\":10}))\n",
    "# {'x': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "def build_df_from_gzipped_s3_folder(\n",
    "    bucket: str,\n",
    "    prefix: str,\n",
    "    header_first_token: str = \"rampid\",\n",
    "    max_keys_per_page: int = 1000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Streams .gz files from s3://{bucket}/{prefix}, finds the header file whose first\n",
    "    non-empty line starts with `header_first_token` (e.g., 'rampid'), uses that as columns,\n",
    "    and concatenates all other files as data rows. No local writes.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # 1) List all objects under the prefix\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    page_iter = paginator.paginate(Bucket=bucket, Prefix=prefix, PaginationConfig={\"PageSize\": max_keys_per_page})\n",
    "\n",
    "    keys = []\n",
    "    for page in page_iter:\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            # Skip \"folders\" and non-gz files if present\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\"/\") or not key.lower().endswith(\".gz\"):\n",
    "                continue\n",
    "            keys.append(key)\n",
    "\n",
    "    if not keys:\n",
    "        raise FileNotFoundError(f\"No .gz objects found under s3://{bucket}/{prefix}\")\n",
    "\n",
    "    # 2) Helper to read first non-empty line from a gzipped S3 object\n",
    "    def _first_nonempty_line(bkt, ky) -> str:\n",
    "        body = s3.get_object(Bucket=bkt, Key=ky)[\"Body\"]\n",
    "        with gzip.GzipFile(fileobj=body) as gz:\n",
    "            with io.TextIOWrapper(gz, encoding=\"utf-8\", newline=\"\") as fh:\n",
    "                for line in fh:\n",
    "                    stripped = line.strip()\n",
    "                    if stripped:\n",
    "                        return stripped\n",
    "        return \"\"  # empty file\n",
    "\n",
    "    # 3) Find header file: first non-empty line begins with 'rampid'\n",
    "    header_key = None\n",
    "    header_line = None\n",
    "    header_token_lower = header_first_token.lower()\n",
    "\n",
    "    for key in keys:\n",
    "        first = _first_nonempty_line(bucket, key)\n",
    "        if first and first.split(\",\")[0].strip().lower().startswith(header_token_lower):\n",
    "            header_key = key\n",
    "            header_line = first\n",
    "            break\n",
    "\n",
    "    if header_key is None or not header_line:\n",
    "        raise ValueError(\n",
    "            f\"Could not find a header file whose first non-empty line starts with '{header_first_token}'.\"\n",
    "        )\n",
    "\n",
    "    # Turn the header line into a list of column names\n",
    "    columns = [c.strip() for c in header_line.split(\",\")]\n",
    "\n",
    "    # 4) Stream and parse all other files as rows (also tolerate if some files repeat header)\n",
    "    dataframes = []\n",
    "    for key in keys:\n",
    "        if key == header_key:\n",
    "            # We already used its first line as header; skip the rest of this file\n",
    "            # (If you actually need to include its data rows below the header line,\n",
    "            # flip this to read the file and skip the first row during parsing.)\n",
    "            continue\n",
    "\n",
    "        body = s3.get_object(Bucket=bucket, Key=key)[\"Body\"]\n",
    "\n",
    "        # Wrap gzip->text stream for pandas\n",
    "        with gzip.GzipFile(fileobj=body) as gz:\n",
    "            # We will read with header=None and assign names=columns.\n",
    "            # Then, drop any accidental header rows where first col == 'rampid'\n",
    "            df = pd.read_csv(\n",
    "                io.TextIOWrapper(gz, encoding=\"utf-8\", newline=\"\"),\n",
    "                header=None,\n",
    "                names=columns,\n",
    "                sep=\",\",\n",
    "                engine=\"python\",\n",
    "                dtype=str,      # safer for mixed/ID-like fields; cast later as needed\n",
    "            )\n",
    "\n",
    "            # Drop accidental header rows present in data files\n",
    "            first_col = df.columns[0]\n",
    "            df = df[df[first_col].str.lower() != header_token_lower]\n",
    "\n",
    "            if not df.empty:\n",
    "                dataframes.append(df)\n",
    "\n",
    "    if not dataframes:\n",
    "        # It's possible all rows were in the header file. If you want to include\n",
    "        # data rows from the header file (after the header row), you can read it too.\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "    # 5) Concatenate\n",
    "    out = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- Example usage ----\n",
    "# df = build_df_from_gzipped_s3_folder(\n",
    "#     bucket=\"my-bucket\",\n",
    "#     prefix=\"path/to/folder/\",\n",
    "#     header_first_token=\"rampid\"\n",
    "# )\n",
    "# print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
