{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd832c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'raw_date': ['Mon, 28th May 2020 12:34:56 GMT', 'Tue, 1st Jan 2019 05:00:00 GMT']\n",
    "})\n",
    "\n",
    "# Remove day suffixes like 'st', 'nd', 'rd', 'th' using regex\n",
    "df['cleaned_date'] = df['raw_date'].apply(lambda x: re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', x))\n",
    "\n",
    "# Parse into datetime format\n",
    "df['parsed_date'] = pd.to_datetime(df['cleaned_date'], format='%a, %d %b %Y %H:%M:%S %Z', errors='coerce')\n",
    "\n",
    "# Optional: just keep the date part (no time)\n",
    "df['just_date'] = df['parsed_date'].dt.date\n",
    "\n",
    "print(df[['raw_date', 'just_date']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "years = ['2010', '2025']\n",
    "car1_prices = [15000, 18000]\n",
    "car2_prices = [20000, 25000]\n",
    "car3_prices = [17000, 22000]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Stacked bars\n",
    "bar1 = ax.bar(years, car1_prices, label='Car 1')\n",
    "bar2 = ax.bar(years, car2_prices, bottom=car1_prices, label='Car 2')\n",
    "\n",
    "# Compute bottoms for car 3 (stacked on car1 + car2)\n",
    "car1_plus_car2 = [c1 + c2 for c1, c2 in zip(car1_prices, car2_prices)]\n",
    "bar3 = ax.bar(years, car3_prices, bottom=car1_plus_car2, label='Car 3')\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_ylabel('Total Price ($)')\n",
    "ax.set_title('Stacked Car Prices in 2010 vs 2025')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40342b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get SHAP interaction values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_interaction_values = explainer.shap_interaction_values(X)  # shape: [n_samples, n_features, n_features]\n",
    "\n",
    "# Define feature of interest\n",
    "feature_name = 'A'\n",
    "feature_index = list(X.columns).index(feature_name)\n",
    "\n",
    "# Interaction values of 'A' with every other feature (averaged across samples)\n",
    "interaction_strengths = np.abs(shap_interaction_values[:, feature_index, :]).mean(axis=0)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "interaction_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'interaction_strength': interaction_strengths\n",
    "}).sort_values(by='interaction_strength', ascending=False)\n",
    "\n",
    "# Optional: Drop self-interaction if you only want cross-feature effects\n",
    "interaction_df = interaction_df[interaction_df.feature != feature_name]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=interaction_df, x='interaction_strength', y='feature')\n",
    "plt.title(f\"Mean SHAP Interaction Strengths with '{feature_name}'\")\n",
    "plt.xlabel(\"Mean |SHAP Interaction Value|\")\n",
    "plt.ylabel(\"Interacting Feature\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12926ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMAKE_ARGS=\"-DLLAMA_CUDA=OFF -DLLAMA_METAL=OFF -DLLAMA_CLBLAST=OFF -DLLAMA_BLAS=OFF\" \\\n",
    "pip install --force-reinstall --no-cache-dir --no-binary llama-cpp-python llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38724e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# --- Step 1: Simulate weekly purchase data ---\n",
    "np.random.seed(0)\n",
    "weeks = pd.date_range(\"2023-01-01\", periods=52, freq='W')\n",
    "states = ['Texas', 'Oklahoma', 'Louisiana', 'California', 'New York']\n",
    "\n",
    "def make_trend(base, noise=5):\n",
    "    return base + np.random.normal(0, noise, size=len(weeks))\n",
    "\n",
    "data = {\n",
    "    'Texas': make_trend(np.linspace(100, 200, 52)),\n",
    "    'Oklahoma': make_trend(np.linspace(90, 180, 52)),\n",
    "    'Louisiana': make_trend(np.linspace(105, 205, 52)),\n",
    "    'California': make_trend(np.linspace(300, 400, 52)),\n",
    "    'New York': make_trend(np.linspace(200, 100, 52)),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=weeks)\n",
    "\n",
    "# --- Step 2: Normalize trends (Z-score) ---\n",
    "df_normalized = df.apply(zscore)\n",
    "\n",
    "# --- Step 3: Hyper-tune K using silhouette score ---\n",
    "X = df_normalized.T  # states as rows\n",
    "sil_scores = {}\n",
    "for k in range(2, min(len(X), 10)):\n",
    "    model = KMeans(n_clusters=k, random_state=0)\n",
    "    labels = model.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    sil_scores[k] = score\n",
    "\n",
    "best_k = max(sil_scores, key=sil_scores.get)\n",
    "print(f\"Best number of clusters: {best_k} with silhouette score: {sil_scores[best_k]:.3f}\")\n",
    "\n",
    "# --- Step 4: Fit final model and visualize ---\n",
    "final_model = KMeans(n_clusters=best_k, random_state=0)\n",
    "labels = final_model.fit_predict(X)\n",
    "cluster_df = pd.DataFrame({'state': X.index, 'cluster': labels})\n",
    "print(cluster_df)\n",
    "\n",
    "# --- Step 5: Plot each cluster's normalized time trends ---\n",
    "for cluster in cluster_df['cluster'].unique():\n",
    "    cluster_states = cluster_df[cluster_df['cluster'] == cluster]['state']\n",
    "    df_normalized[cluster_states].plot(title=f\"Cluster {cluster} Trends\")\n",
    "    plt.ylabel(\"Z-Score Normalized Purchases\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Step 6: Correlation heatmap ---\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Raw Correlation of Purchase Trends\")\n",
    "plt.show()\n",
    "\n",
    "# --- Step 7: Intra-cluster correlation check ---\n",
    "def mean_cluster_corr(corr_matrix, cluster_df):\n",
    "    results = {}\n",
    "    for cluster in cluster_df['cluster'].unique():\n",
    "        members = cluster_df[cluster_df['cluster'] == cluster]['state']\n",
    "        intra = corr_matrix.loc[members, members].values\n",
    "        intra_corr = intra[np.triu_indices_from(intra, k=1)]\n",
    "        results[f\"Cluster {cluster} mean corr\"] = np.mean(intra_corr)\n",
    "    return results\n",
    "\n",
    "print(mean_cluster_corr(corr, cluster_df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
